---
title: "Generating GIAB BAM Metadata Table"
date: '`r Sys.Date()`'
author: "ND Olson"
format: html
---

```{r setup}
#| include=FALSE,
#| echo=TRUE
library(tidyverse)
library(here)
```

# Summary

<!-- Brief overview of analyses performed. Aim for three to five sentences, with the following parts: -->

<!-- 1.  **WHY** the analysis was performed - summary of Context and Objective sections below.\ -->

<!-- 2.  **HOW** the analysis was performed - summary of Approach section below.\ -->

<!-- 3.  **WHAT** was the outcome of the analysis - summary of the Analyses and Conclusions sections below. -->

Sanity checks and generating combined metadata tables for fastq, bam, and vcf using tables generated by the giab-metadata-stack, manually generated tables, and SeqSleuth.

# Background

<!-- Outline or write a rough draft of background section text before starting the analysis. Providing appropraite level of detail for the analysis scope and target audience (could be collaborator, supervisor, or future you.) After analysis is complete revise section as needed and appropriate. -->

Tables generated GIAB Stack - bam_metadata.csv - bamstats.csv - error_fastq.csv - fastq_metadata.csv - fastqstats.csv - ftp.csv -

## Context

<!-- General background information for reader to understand **WHY** the analysis was performance. -->

## Objective

<!-- The objective statement provides an explicit description of the questions (related to the **WHY** ) you aim to address with the analyses. -->

1.  load the different individual data tables
2.  perform initial sanity checks
3.  Combine individual tables
4.  additional sanity checks
5.  Joining with appropriate manually generated tables
6.  documentation - generating data dictionaries for data tables

## Approach

<!-- Brief description can be 1 or more sentences or bullet points with **HOW** you plan to perform accomplish the objective. After the analyses is complete revise as appropriate so that it reflects **HOW** the objective was addressed. This section is equivalent to a manuscript methods section. For a formal analyses this section can be the starting point for relevant methods sections. -->

Manually generated tables obtained from googlesheets using code in the `get_manually_generated_sheets.qmd`

# Analyses

<!-- The analyses goes here and can use the following structure. This is the **WHAT** was done part of the analyses -->

## Load and Tidy Data

<!-- Reading input files and transform into tidy data structures for exploratory and statistical analyses. Make sure to document reasoning behind data transformations and manipulations. The code provides documentation on how the transformations were performed.  -->

Loading metadata tables

```{r}
ftp_tbl <- read_csv(here("raw-data/ftp.csv"))
s3_tbl <- read_csv(here("raw-data/s3.csv"))
bamstats_tbl <- read_csv(here("raw-data/bamstats.csv"))
bam_metadata <- read_csv(here("raw-data/bam_metadata.csv"))
bam_parsed_metadata <-
    jsonlite::fromJSON(here("raw-data/full_bam_metadata.json.gz"))
```

Loading previously generated metadata table

```{r}
bam_metadat_20230518 <- here("metadata_tables/bam_metadata.csv") %>% 
    read_csv()
```

```{r}
glimpse(bam_metadat_20230518)
```

Large number of NAs is due to columns that were added from manually generated tables. 
Will want to have Jennifer work on plots that are not dependent on values from manually generated tables.
```{r}
bam_metadat_20230518 %>% filter(!is.na(coverage))
```

```{r}

```

## Results

### Table Summaries
```{r}
glimpse(bam_metadata)
```

```{r}
glimpse(bam_parsed_metadata)
```

```{r}
glimpse(bamstats_tbl)
```

```{r}
glimpse(s3_tbl)
```

### Joining Tables

```{r}
make_stats_df <- function(stats_str){
    # Remove unwanted characters and split into a vector
    str_vector <- str_remove(stats_str, '^\\"\\[') %>% 
        str_remove('\\]\\"$')
    str_vector <- str_remove_all(str_vector, "IndexStats\\(")
    str_vector <- str_remove_all(str_vector, "\\)")
    str_vector <- str_split(str_vector, "\\s", simplify = TRUE)
    
    # Convert to dataframe and rename columns
    df <- as.data.frame(t(str_vector))
    print(df)
    # names(df) <- c("contig", "mapped", "unmapped", "total")
    
    # Cleanup and convert to appropriate data types
    # df %>%
    #   mutate(contig = str_remove_all(contig, "contig='|'"),
    #          mapped = as.numeric(str_remove_all(mapped, "mapped=")),
    #          unmapped = as.numeric(str_remove_all(unmapped, "unmapped=")),
    #          total = as.numeric(str_remove_all(total, "total=")))
}

bamstats_tbl$stats[3] %>% make_stats_df()
```
```{r}
jsonlite::fromJSON("[IndexStats(contig='chrM', mapped=2, unmapped=0, total=2), IndexStats(contig='chr1', mapped=89, unmapped=11, total=100)]")
```


### Sanity Checks 


### Writing to file
```{r}
fs::dir_create("metadata_tables")
write_csv(bam_metadat, "metadata_tables/bam_metadata.csv")
```

## Conclusions

<!-- -   Brief concluding remarks about the analyses performed. -->

<!-- -   Any bugs or issues identified while performing the analyses. -->

# Session Information

<!-- The following sections (in addition to the header) provide the **WHO** and **WHERE** for the analyses was performed. -->

## System Information

```{r}
sessioninfo::platform_info()
```

## Package Versions

```{r}
sessioninfo::package_info() %>% 
    filter(attached = TRUE) %>% 
    select(package, loadedversion, date, source) %>%
    knitr::kable(booktabs = TRUE, row.names = FALSE)
```