---
title: "Generating GIAB Metadata Tables"
date: '`r Sys.Date()`'
author: "ND Olson"
format: html
editor: visual
---

```{r setup}
#| include=FALSE,
#| echo=TRUE
library(tidyverse)
library(here)
```

# Summary

<!-- Brief overview of analyses performed. Aim for three to five sentences, with the following parts: -->

<!-- 1.  **WHY** the analysis was performed - summary of Context and Objective sections below.\ -->

<!-- 2.  **HOW** the analysis was performed - summary of Approach section below.\ -->

<!-- 3.  **WHAT** was the outcome of the analysis - summary of the Analyses and Conclusions sections below. -->

Sanity checks and generating combined metadata tables for fastq, bam, and vcf using tables generated by the giab-metadata-stack.

# Background

<!-- Outline or write a rough draft of background section text before starting the analysis. Providing appropraite level of detail for the analysis scope and target audience (could be collaborator, supervisor, or future you.) After analysis is complete revise section as needed and appropriate. -->

## Context

<!-- General background information for reader to understand **WHY** the analysis was performance. -->

## Objective

<!-- The objective statement provides an explicit description of the questions (related to the **WHY** ) you aim to address with the analyses. -->

1.  load the different individual data tables
1.  perform initial sanity checks
1.  Combine individual tables
1.  additional sanity checks
1.  Joining with appropriate manually generated tables
5.  documentation - generating data dictionaries for data tables

## Approach

<!-- Brief description can be 1 or more sentences or bullet points with **HOW** you plan to perform accomplish the objective. After the analyses is complete revise as appropriate so that it reflects **HOW** the objective was addressed. This section is equivalent to a manuscript methods section. For a formal analyses this section can be the starting point for relevant methods sections. -->

Manually generated tables obtained from googlesheets using code in the `get_manually_generated_sheets.qmd`

# Analyses

<!-- The analyses goes here and can use the following structure. This is the **WHAT** was done part of the analyses -->

## Load and Tidy Data

<!-- Reading input files and transform into tidy data structures for exploratory and statistical analyses. Make sure to document reasoning behind data transformations and manipulations. The code provides documentation on how the transformations were performed.  --> Loading metadata tables

```{r}
ftp_tbl <- read_csv("raw-data/ftp.csv")
s3_tbl <- read_csv("raw-data/s3.csv")
bamstats_tbl <- read_csv("raw-data/bamstats.csv")
bam_metadata <- read_csv("raw-data/bam_metadata.csv")
vcfstats_tbl <- read_csv("raw-data/vcfstats.csv")
fastqstats_tbl <- read_csv("raw-data/fastqstats.csv")
fastq_metadata <- read_csv("raw-data/fastq_metadata.csv")
vcf_metadata <- read_csv("raw-data/vcf_metadata.csv")
```

## Results

```{r}
ftp_tbl %>% count(filetype)
```

```{r}
s3_tbl %>% count(filetype)
```

### Fastq Sanity checks

Total fastqs from ftp and s3

```{r}
47345 + 555
```

The fastq metadata tables has 10,559 entries, which is the same as the fastqstats table. - This is consistent with expectations as we excluded a large number of smaller files from the file list - There were a few fastqc errors will want to examine the fastq error table to see if they are fixable and worth trying to fix.

```{r}
glimpse(fastq_metadata)
```

### Bam sanity checks AND table joining

The ftp site has 3,641 bam files and the s3 site has 61 bam files. Stats were not collected for the bam files on the s3 site because unable to stream files from s3 for samtools stats. I was unable to perform a join on the bamstats and ftp tables

The bamstats_tbl has 2096 entries

```{r}
glimpse(bamstats_tbl)
```

```{r}
glimpse(ftp_tbl)
```

```{r}
bam_tbl <- bamstats_tbl #%>% select(-aligner, -stats, -md5key)


ftp_tbl %>% 
    right_join(bam_tbl) %>% 
    glimpse()
```


__Note__ Work previously done before manually generated table downloading and merging.
Generating table for interns to get aligner information

```{r}
mg_tables <- list.files("raw-data/manually_generated_tables",
                        pattern = "alignments",
                        recursive = TRUE, full.names = TRUE)

tech_names <- str_extract(mg_tables,
                          "(?<=manually_generated_tables/).*(?=/)")

mg_alignment_df <- mg_tables %>% 
    set_names(tech_names) %>% 
    map_dfr(read_tsv, .id = "tech")
```

```{r}
glimpse(mg_alignment_df)
```

```{r}
bam_metadata2 <- bam_metadata %>% 
    mutate(ftp_bam_file = glue::glue("https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples{str_remove(filepath,'/ftp')}/{bam}")) %>% 
    rename(aligner_cmd = aligner,
           giab_id_cdk = giab_id,
           trio_cdk = trio,
           notes_cdk = notes) %>% 
    full_join(select(mg_alignment_df,-md5key))
```

```{r}
glimpse(bam_metadata2)
```


```{r}
bam_metadata2 %>% filter(!is.na(tech))
```
```{r}
dups <- bam_metadata2 %>% count(ftp_bam_file) %>% filter(n > 1)
bam_metadata2 %>% right_join(dups)
```

```{r}
fs::dir_create("metadata_tables")
write_csv(bam_metadata2, "metadata_tables/bam_metadata.csv")
```


### VCF sanity checks

There are 2268 vcfs on the ftp site and 69 on s3. The vcf stats table has 1,176 entries No vcf error crawler - will want to generate one checking for vcf stat errors

```{r}
vcf_metadata %>% glimpse()
```

```{r}
mg_tables <- list.files("raw-data/manually_generated_tables",
                        pattern = "variants",
                        recursive = TRUE, full.names = TRUE)

tech_names <- str_extract(mg_tables,
                          "(?<=manually_generated_tables/).*(?=/)")

mg_variants_df <- mg_tables %>% 
    set_names(tech_names) %>% 
    map_dfr(read_tsv, .id = "tech")
```
```{r}
vcf_metadata2 <- vcf_metadata %>% 
    mutate(ftp_vcf_file = glue::glue("https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples{str_remove(filepath,'/ftp')}/{str_remove(location, '.*/')}")) %>% 
    rename(giab_id_cdk = giab_id) %>% 
    full_join(select(mg_variants_df, -md5key))
```

Additional VCFs in manually generated tables from pFDA challenge - currently not on ftp site or included in cdk generated table and therefore will exclude from combined table.
```{r}
vcf_metadata2 %>% filter(!is.na(tech), is.na(md5key))
```

```{r}
vcf_metadata2 <- vcf_metadata %>% 
    mutate(ftp_vcf_file = glue::glue("https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples{str_remove(filepath,'/ftp')}/{str_remove(location, '.*/')}")) %>% 
    rename(giab_id_cdk = giab_id) %>% 
    left_join(select(mg_variants_df, -md5key))
```

```{r}
write_csv(vcf_metadata2, "metadata_tables/vcf_metadata.csv")
```


## Conclusions

<!-- -   Brief concluding remarks about the analyses performed. -->

<!-- -   Any bugs or issues identified while performing the analyses. -->

# Session Information

<!-- The following sections (in addition to the header) provide the **WHO** and **WHERE** for the analyses was performed. -->

## System Information

```{r}
sessioninfo::platform_info()
```

## Package Versions

```{r}
sessioninfo::package_info() %>% 
    filter(attached = TRUE) %>% 
    select(package, loadedversion, date, source) %>%
    knitr::kable(booktabs = TRUE, row.names = FALSE)
```
